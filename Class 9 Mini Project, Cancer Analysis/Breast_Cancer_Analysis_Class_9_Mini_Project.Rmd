---
title: "Class 9 Mini Project- Breast Cancer Cell Analysis"
author: "Vera Sophia Beliaev"
date: "2/13/2022"
output: pdf_document
---

# Exploratory Data Analysis

Preparing the data
```{r}
fna.data <-  "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)
View(wisc.df)
head(wisc.df)
```
Wisc.df has "diagnosis" column which indicates whether a sample is benign or malignant.
```{r}
#Create new data frame that omits diagnosis column
wisc.data <- wisc.df[,-1]
```

```{r}
#Create vector w/ diagnoses
diagnosis <- factor(wisc.df$diagnosis)
diagnosis
```
> Q1. How many observations are in this dataset?

There are 569 observations in wisc.data .
```{r}
str(wisc.data)
```
> Q2. How many of the observations have a malignant diagnosis?

212 of the observations have a malignant diagnosis.
```{r}
table(diagnosis)
```
> Q3. How many variables/features in the data are suffixed with _mean?

10 variables in the data are suffixed with _mean.
```{r}
#names() allows us to get the names of the objects in wisc.data
length(grep("_mean", names(wisc.data)))
```

# Principal Component Analysis

```{r}
#Check column means
colMeans(wisc.data)
```

```{r}
#apply the sd function to the columns of wisc.data
#MARGIN arg: 1 = rows, 2 = cols, c(1,2) = rows & cols
apply(wisc.data, 2, sd)
```

```{r}
#Perform PCA with prcomp() fn
wisc.pr <- prcomp(wisc.data, scale=TRUE)

```

```{r}
#PCA results
summary(wisc.pr)
```
> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

The proportion of the original variance captured by the first principal component (PC1) is 44.27%

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

At least 3 PCs are required to describes at least 70% of the original variance in the data.

> Q6.  How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

At least 7 PCs are required to describe at least 90% of the original variance in the data.

# Interpreting PCA Results

```{r}
#Scree plot
plot(wisc.pr, main= "Quick scree plot")
```

```{r}
#Biplot
biplot(wisc.pr)
```
> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

The biplot is not easy to interpret. It is cluttered and unclear.

```{r}
#Scatterplot w/ PC1 & PC2, points colored by dx
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis, xlab = "PC1", ylab = "PC2")

```

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

The plots are very similar however plotting PC1 vs PC3 shifts the plotted points downwards visually. Overall, there does not seem to be too much difference between the two plots.


## Plot with ggplot2
```{r}
library(ggplot2)
```

ggplot uses data frames for input therefore the diagnosis vector will be converted into a col

```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
```


```{r}
#Scatter plot colored by dx
ggplot(df) + aes(PC1, PC2, col=diagnosis) + geom_point()
```


# Variance explained

```{r}
#The variance of e/ component
pr.var <-  wisc.pr$sdev^2
head(pr.var)
```

```{r}
#Variance explained by e/ PC
pve <-  pr.var/sum(pr.var)
```

```{r}
# Plot variance explained by each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

It is -0.26085376
```{r}
wisc.pr$rotation[,1]
```
>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

At least 5 principal components are required to explain 80% of the variance of the data.

# Using alt programs for PCA (ade4 and factoextra)

```{r}
#Scree plot
library(ade4)
dudipca <- dudi.pca(df = wisc.data, center = TRUE, scale = TRUE, scannf = FALSE, nf = 30)
dudipca
```

```{r}
library(factoextra)
```

```{r}
#Scree plot
fviz_eig(dudipca, addlabels=TRUE)
```
The above Scree plot shows PC1 containing 44.3% of variance, PC2 containing 19%, PC3 containing 9.4% and so on

```{r}
get_eig(dudipca)
```

```{r}
fviz_pca(dudipca)
```
Above is a visually complicated biplot

```{r}
fviz_pca_var(dudipca, col.var="contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Avoid text overlapping
             )
```

```{r}
fviz_pca_ind(dudipca, col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Avoid text overlapping (slow if many points)
             )
```

```{r}
fviz_pca_ind(dudipca,
             label = "none", # hide individual labels
             habillage = wisc.df$diagnosis, # color by groups
             palette = c("#00AFBB", "#E7B800"),
             addEllipses = TRUE # Concentration ellipses
             )
```

# Hierarchical Clustering

```{r}
#Scale the data
data.scaled <- scale(wisc.data)
```

```{r}
#Calc Euclidean dists b/w pairs of observations
data.dist <-  dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```
> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

The height at which the clustering model has 4 clusters is approximately 18.
```{r}
plot(wisc.hclust)
abline(h=18, col= "red", lty=2)
```

```{r}
#Cut the tree
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```

```{r}
#COmpare our clusters to actual diagnoses
table(wisc.hclust.clusters, diagnosis)
```
This showed that cluster 1 has mostly malignant cells, and cluster 3 has mostly benign cells.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=3)
table(wisc.hclust.clusters, diagnosis)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=2)
table(wisc.hclust.clusters, diagnosis)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=5)
table(wisc.hclust.clusters, diagnosis)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=6)
table(wisc.hclust.clusters, diagnosis)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=10)
table(wisc.hclust.clusters, diagnosis)
```
> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

No, 4 clusters seems to be the best match.

```{r}
#Create plots using, single, complete, average, and ward.D2
plot(hclust(data.dist, method = "single"))
plot(hclust(data.dist, method = "complete"))
plot(hclust(data.dist, method = "average"))
plot(hclust(data.dist, method = "ward.D2"))
```
> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

Using the ward.D2 method seems to give the "cleanest" looking dendogram. The runner up is "complete."

# K Means Clustering

```{r}
wisc.km <-  kmeans(wisc.data, centers=2, nstart = 20)
```

```{r}
table(wisc.km$cluster, diagnosis)
```

> Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

The two diagnoses seem to have a well-defined split using k-means. Overall, it is similar to the hclust results when k=4.

```{r}
table(wisc.km$cluster,cutree(wisc.hclust, k=4))
```

# Combining Methods

Clustering on PCA Results

```{r}
wisc.pr.hclust <- (hclust(data.dist, method = "ward.D2"))
```

```{r}
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
#plot w/ reordered factor
plot(wisc.pr$x[,1:2], col=g)
```

```{r}
#clustering w/ 1st 7 PCs
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```

> Q15. How well does the newly created model with two clusters separate out the two diagnoses?

The two diagnoses appear to be separated significantly by using the new model with two clusters.

> Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

The k-means and clustering models are roughly good at separating the diagnoses but give ballpark results.

```{r}
table(diagnosis)
```

```{r}
table(cutree(wisc.hclust, k=4), diagnosis)
```

```{r}
table(wisc.km$cluster, diagnosis)
```

# Sensitivity/Specificity

Sensitivity refers to a test’s ability to correctly detect ill patients who do have the condition. In our example here the sensitivity is the total number of samples in the cluster identified as predominantly malignant (cancerous) divided by the total number of known malignant samples. In other words: TP/(TP+FN).

Specificity relates to a test’s ability to correctly reject healthy patients without a condition. In our example specificity is the proportion of benign (not cancerous) samples in the cluster identified as predominantly benign that are known to be benign. In other words: TN/(TN+FN).

```{r}
#hclust 

#sensitivity
165/212

#Specificity
343/357
```

```{r}
#k-means

#Sensitivity
130/212

#Specificity
356/357
```

> Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

K-means gives the best specificity while hclustering gives the best sensitivity.

# Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q18. Which of these new patients should we prioritize for follow up based on your results?


Patient 2 (located in the red/malignant cluster) should be prioritized for follow-up.

